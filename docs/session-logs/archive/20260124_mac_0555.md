# Session Recap: mac_20260124_0555
**Time:** 2026-01-24 05:55 EST
**Terminal:** mac
**Est. Tokens:** ~30,000

## Completed
- Built Hailo-Ollama from source on Raspberry Pi 5 with Hailo-10H NPU
- Installed hailo-ollama binary to ~/.local/bin/
- Created systemd service for auto-start on boot
- Pulled and installed all 5 available LLM models:
  - qwen2.5:1.5b (2.37 GB) - General chat
  - qwen2.5-coder:1.5b (1.68 GB) - Code generation
  - qwen2:1.5b (1.70 GB) - Legacy Qwen
  - deepseek_r1:1.5b (2.21 GB) - Reasoning/math
  - llama3.2:1b (1.88 GB) - Fast lightweight tasks
- Verified Hailo-Ollama REST API working on http://192.168.4.147:8000
- Tested chat completions with multiple models
- Documented n8n integration endpoints (Ollama and OpenAI compatible)

## Decisions Made
- Used Hailo-Ollama instead of standard Ollama for NPU acceleration
- Installed all 5 available models (~9.8 GB total) for flexibility in n8n workflows
- Set up systemd service for persistence across reboots

## New Open Items
- Create n8n workflow using local Hailo LLM endpoint
- Consider installing Open WebUI for browser-based chat interface
- Monitor token throughput (~6 tok/s observed, expected ~30 tok/s)

## Notes
- Hailo-10H NPU: 40 TOPS INT4, 8GB dedicated LPDDR4X RAM
- Pi 5: 16GB system RAM, Cortex-A76 CPU
- Server accessible via Tailscale from any device on network
- API endpoints: /api/chat (Ollama), /v1/chat/completions (OpenAI)
